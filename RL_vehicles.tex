
% Bug: automatically loads natbib with name options, cannot be
% overridden, `Elsevier LaTeX' style produces error messages

\documentclass[review]{elsarticle}
%\documentclass{elsarticle}

\usepackage{hyperref}
\usepackage{lineno,hyperref} \modulolinenumbers[5]
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{units}

\providecommand{\red}[1]{\textcolor{red}{#1}}
\providecommand{\blue}[1]{\textcolor{blue}{#1}}
\providecommand{\green}[1]{\textcolor{green}{#1}}

\usepackage{color}
\providecommand{\martin}[1]{\red{#1}} %Preprint
%\providecommand{\martin}[1]{}                  %fuer Veroeffentlichung


\journal{Transportation Research Part C}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}
 
%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
% \bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frontmatter}

\title{Formulation and validation of a car-following model based on deep
  reinforcement learning}

%% Group authors per affiliation:
%\author{Fabian Hart\fnref{myfootnote}}
%\address{TU Dresden}
%\fntext[myfootnote]{Comment.}

%% or include affiliations in footnotes:
\author[firstAddress]{Fabian Hart}
\author[firstAddress,secondAddress]{Ostap Okhrin}
\author[firstAddress,secondAddress]{Martin Treiber\corref{corrAuthor}}
\cortext[corrAuthor]{Corresponding author}
\ead{Martin.treiber@tu-dresden.de}
\ead[url]{www.mtreiber.de}

\address[firstAddress]{TU Dresden}
\address[secondAddress]{Possible second address}




\begin{abstract}
To be written at the end
\end{abstract}

\begin{keyword}
reinforcement learning \sep car-following model \sep stochastic
processes \sep string stability \sep validation \sep trajectory data 
\end{keyword}

\end{frontmatter}

%\linenumbers

\section{Introduction}

Autonomous driving technologies are seen as promising solutions to improve road safety, where human errors account for 94\% of the total accidents \cite{vehicleCrashSurvey2015}. Moreover congestion, energy consumption and emissions are intended to be reduced. 
But autonomous driving is a challenging task since the transportation traffic can be dynamic and unpredictable.
On the way to fully autonomous driving, Advanced Driver Assistance Systems
(ADAS) has been developed to solve tasks like lane-keeping, lane-changing, car-following, emergency braking, etc.
Since Deep Learning methods has been demonstrated to surpass humans in certain domains, they are also adopted in the area of autonomous driving.
Especially Deep Reinforcement Learning (DRL), which combines the power of Deep Learning in tackling large, complicated problems with Reinforcement Learning, has shown its potential in a broad variety of autonomous driving tasks. 
In \cite{OnRampMerge2018} and \cite{OnRampMerge2020}, DRL is used to guide an autonomous vehicle safely from on-ramp to freeway. In \cite{intersection1}, \cite{intersection3} and \cite{intersection2}, DRL methods are used to manage traffic of autonomous vehicles at intersections, optimizing safety and efficiency.
In \cite{LangeChange1}, DRL is used to solve lane change maneuvers.

A further task in autonomous driving is to model the vehicle behavior under car following scenarios, where suitable accelerations has to computed in order to achieve a safe and comfortable driving. Approaches to solve this task are classical car-following models, such as the Intelligent Driver Model \cite{Opus} or stochastic car-following models, such as in \cite{Treiber2018stochIDM_TRB}. Furthermore data-driven approaches use Machine Learning methods to train a car-following model based on experimental car-follower data, such as in \cite{Chong2011SimulationOD} or \cite{ZHOU2017245}. Downside of this approach is that the model tries to emulate human driver behavior which still can be suboptimal.

To overcome this issue, DRL methods train non-human car-following models that can optimize metrics such as safety, efficiency and comfort. 
One approach is to train on scenarios, where the leading vehicles trajectory, used for training, is based on experimental data, such as in \cite{SafeEfficientAndComfortable} or \cite{HumanLikeAutonomouCF}. Similar approaches suggest a standardized driving cycle, which functions as leading vehicle trajectory, such as \cite{ComparisonRLvsMPC} or \cite{CFelectricVehicle}, which uses the New European Driving Cycle.
A disadvantage coming along with these approaches is, that for scenarios, which are not in the scope of the training data, the performance decreases, indicating inadequate machine learning generalization \cite{ComparisonRLvsMPC}. 

Another issue of car-following models is string-stability. There are several studies focusing on dampening traffic oscillations by using a sequence of trained vehicles, such as \cite{qu2020jointly}, \cite{DissipatingStopAndGoWaves} and \cite{DampenStopAndGoTraffic}.

All the mentioned DRL car-following models have three disadvantages in common: At first the acceleration range is limited in a way, that full-brakes are not considered. This results in models that are just designed for non-safety-critical scenarios. Second these models just consider car-following scenarios, while free driving or the transition between both is not reflected in the reward function. Third the trained models have just been validated on data that is similar to the training data set, so that the generalization capabilities cannot be proved. 

This motivated us to design a model, which overcomes these issues.
To our knowledge, no RL based car-following model has been proposed which has the following features combined:


\begin{itemize}
	\item  The proposed model considers free driving, car-following, as well as the transition between both in a way, that approaching of the leading vehicle is smooth and comfortable.
	\item The proposed model has a wider range of possible accelerations, which leads to a collision-free behavior also in safety-critical situations such as full-braking of the Leader Vehicle.
	\item The proposed model is trained on leading trajectories, based on an AR(1)-process, e. g. \cite{HonerkampEngl}, the parameters reflecting kinematics of real drivers. This leads to high generalization capabilities and a model usage in a broader variety of traffic scenarios. 
	\item Different driver characteristics can be modeled by adjusting the parameters of the reward function.
	\item The proposed model shows string-stability.
	
\end{itemize}

Another feature of this work is a thorough validation of the above mentioned properties in scenarios based on both synthetic and naturalistic trajectory data, bringing the model to its limits. 
In all cases, the model proved to be accident free and string stable.
In a further experiment the proposed model is compared to an Intelligent Driver Model, calibrated on the same data. The results indicate a better performance of the proposed model.


[short textual enumeration of the sections to come]

\section{Model specification}
The \martin{following vehicle} \martin{[only named things are written
    in uppercase; special concepts/definitions such as Reinforcement Learning
    can be written both upper and lowercase; abbreviation always
    uppercase (RL); the
    ``reward function'' is generic, hence lowercase]} is controlled by a Reinforcement
Learning (RL) agent. By interaction with an environment, the RL agent
optimizes a sequential decision making problem. At each time step
$t$\martin{, [always comma before the main clause starts]} the agent observes an environment state $s_t$ and, based on that state, selects an action $a_t$. After conducting action $a_t$, the RL agent receives a reward $r(a_t,s_t)$. The agent aims to learn an optimal state-action mapping policy $\pi$ that maximizes the expected accumulated discounted reward $r_{t}=\sum_{k=0}^{\infty} \gamma^{k} r_{t+k}$, with $\gamma = (0,1]$ describing the discount factor. The crucial elements
of the RL-based \martin{[use your abbreviation]} control strategy are described
in detail as follows:

\subsection{Action space}
\label{actionSpace}
In this study, the acceleration of the following vehicle is considered
as the action of the RL agent. To enable comfortable driving and
\martin{allow for a maximum braking deceleration} in safety-critical
situations, the acceleration \martin{is a continuous variable in the
  range} between $a_{\rm min} = \unit[-9]{m/s^2}$ and 
$a_{\rm max} =\unit[2]{m/s^2}$.\martin{[In contrast to variables or symbolic
    constants which are set in italic, units are set in roman
    (upright) font. If super- or subscripts are variables, they are
    set italic. If they are abbreviations (e.g., min, max) they are set
    upright. Multi-letter standard function names such as $\sin$,
    $\tan$, $\cosh$ or $\exp$ are written upright (use
    \texttt{\$\textbackslash sin\$} etc)]}


\subsection{State space}
The state space defines the observations \martin{that} the following
vehicle can receive from the environment. To compute an optimal
acceleration, the following vehicle observes its own acceleration $a$,
its own \martin{speed [velocity=vector, speed=abs(velocity)=scalar]}
$v$, the \martin{relative speed $\Delta v=v_l-v$ where $v_l$ denotes
  the speed of the leading vehicle [crucial since both definitions
    $v_l-v$ or $v-v_l$ are common]} and the \martin{(bumper-to-bumper)
  gap $s$ to the leader [again, some use the space headway (including
    the leading vehicle length), and some the gap]}. These
observations are normalized to the range $[-1,1]$. \martin{[How? If it
    is just linear translation and scaling, you need min and max
    values of the state variables. How to determine them?]}


\subsection{Reward Function}
\label{rewardFunction}
The goal of the RL strategy is to reduce the crash risk, while
maintaining comfortable driving in non-safety-critical situations
\martin{and minimizing the travel time [without that, creeping which
    is safe and comfortable would be the optimum strategy]}. The
\martin{r}eward function includes a set of parameters that can be
adjusted to realize different \martin{driving} styles. $v_{\rm des}$
is the desired velocity that should not be exceeded. $a_{\rm min}$ and $a_{\rm max}$ are the minimum and maximum possible accelerations, as described in Section \ref{actionSpace}. All parameters are described in Table \ref{tab:agentParameters}.

\begin{table}
\caption{RL agent parameters and default values} 
\label{tab:agentParameters} 
\begin{center}
\begin{tabular}{ p{0.12\textwidth}| p{0.65\textwidth}| p{0.1\textwidth}}
	Parameter & Description & Value \\ \hline
	$a_{\rm min}$ & Minimum acceleration & $-9m/s^2$ \\  
	$a_{\rm max}$ & Maximum acceleration & $2m/s^2$ \\  
	$b_{\rm comf}$ & Comfortable deceleration & $-2m/s^2$ \\  
	$v_{\rm des}$ & Desired velocity & $16.6 m/s$ \\  		
	$T_{\rm gap}$ & Desired time gap to Leader & $1.5s$ \\
	$s_{\rm min}$ & Desired minimum space gap to Leader & $2m$ \\
	$T_{\rm var}$ & Time gap to describe the variance of the normal probability function (see Equation \ref{eq:r1} - \ref{eq:r13}) & $0.7s$ \\
	$T_{\rm lim}$ & Upper time gap limit for zero reward (see Equation \ref{eq:r1} - \ref{eq:r13}) & $15s$ 
\end{tabular}
\end{center}
\end{table}


The reward function consists of four parts, described as follows:
\martin{[The standard normal distribution has the symbol $\Phi(.)$ and
    its density $\phi(.)$ or $\varphi(.)$; I prefer $\varphi$ since, then,
    the optical difference to the CDF $\Phi$ is greater]}

\begin{equation}
\label{eq:r1}
r_1  = 
\begin{cases}
\frac{\varphi(s,  s_{\rm opt},  s_{\rm var})}{\varphi( s_{\rm opt},  s_{\rm opt},  s_{\rm var})},& \text{if } s < s^*\\
\frac{\varphi(s^*,  s_{\rm opt},  s_{\rm var})}{\varphi( s_{\rm opt},  s_{\rm opt},  s_{\rm var})} \left(1-\frac{s-s^*}{s_{\rm lim} - s^*}\right)              & \text{otherwise}
\end{cases}
\end{equation}
\martin{[Use \texttt{\textbackslash left( ... \textbackslash right)}
    to make ``bigger'' parentheses, brackets, braces around fractions
    or super- and subscript quantities]}
with
\begin{equation}
\label{eq:r11}
s_{\rm opt} = vT_{\rm gap} + s_{\rm min},
\end{equation}
\begin{equation}
\label{eq:r12}
s_{\rm var} = vT_{\rm var} + 0.5s_{\rm min},
\end{equation}
\begin{equation}
\label{eq:r13}
s_{\rm lim} = vT_{\rm lim} + 2s_{\rm min},
\end{equation}
%
and $\varphi(x,\mu,\sigma^2)$ describing a normal probability density
function. \martin{[No empty line if no new paragraph; otherwise, you get an
  indent that looks strange] [$s^*$ is not defined]}.

The first part of the reward function aims to maintain a reasonable distance to the Leader Vehicle. Figure \ref{fig:RewardFunc1} illustrates the reward function for $r_1$, containing the parameter $s_{\rm opt}$, $s^*$ and $s_{\rm lim}$. The reward function is designed in a way, that for high velocities $v$ of the following vehicle the time gap between Follower and Leader Vehicle tends to $T_{\rm gap}$, while for low velocities the distance between both tends to $s_{\rm min}$. Different values of $T_{\rm opt}$ result in different driving styles in a way, that for higher values of $T_{\rm opt}$ the Follower drives up closer, resulting in a more aggressive driving style. The results for different values of $T_{\rm opt}$ can be found in Section \ref{sec:differentT}. Different functions for $ s > s^*$ has been applied, but the best results regarding a smooth and comfortable approaching of the following vehicle has been reached with a linear function. Also, a high value of $T_{\rm lim}$ results in an early deceleration and comfortable approaching. 

\begin{figure}
	\centering
	\includegraphics[width=12cm]{images/RewardFunc1}
	\caption{Reward function part 1 maximizes the reward for car following with time gap $T_{\rm gap}$}
	\label{fig:RewardFunc1}
\end{figure}


\begin{equation}
r_2 = 
\begin{cases}
\tanh\left(\frac{b_{kin}-b_{\rm comf}}{a_{\rm min}}\right),& \text{if } b_{kin}>b_{\rm comf}\\
0,              & \text{otherwise}
\end{cases}
\end{equation}

with
\begin{equation}
b_{kin} = \frac{\Delta v^2}{s}
\end{equation}
The second part of the reward function addresses the vehicle behavior in safety-critical situations.
For a deceleration with the delay $b_{kin}$ the braking distance is equal to the current distance $s$. Thus, the kinematic deceleration $b_{kin}$ represents the minimum deceleration necessary to avoid a collision. A situation is considered as "safety-critical", if the kinematic deceleration $b_{kin}$ is greater than the comfortable deceleration $b_{\rm comf}$. Thus, just in safety-critical situations the RL agent is getting penalized, illustrated in figure xy.

\begin{equation}
r_3 = -\left(\dfrac{da}{dt}\right)^2
\end{equation}

The third part of the reward function aims to reduce the jerk in order to achieve comfortable driving. 

\begin{equation}
r_4 =  
\begin{cases} 
 -min\left(1,\left( v - v_{\rm des}\right)^2\right), & \text{if } v>v_{\rm des}\\
0, & \text{otherwise}
\end{cases}             
\end{equation}

The fourth part of the reward function penalizes the RL agent, if the current velocity $v$ is above the desired velocity $v_{\rm des}$. 

\begin{equation}
r = 0.6r_1 + 1.1r_2 + 0.001 r_3 + r_4
\end{equation}

The weights of each reward part has been found experimentally and can further be optimized in future studies.

\subsection{RL algorithm}
In various similar control problems, the Deep Deterministic Policy Gradient (DDPG) Algorithm has been used and proven to perform well on tasks with a continuous action and state space, such as in \cite{SafeEfficientAndComfortable}, \cite{ComparisonRLvsMPC} or \cite{HumanLikeAutonomouCF}. The original work can be found in \cite{DDPG}. In order to reduce the variance of policy gradients and increase learning speed, DDPG is an actor-critic method. The actor determines the action, while the critic judges about the quality of the action and how the policy should be adjusted. In this study, both networks are feed-forward neural networks with two layers of hidden neurons and 32 neurons each hidden layer. All DDPG parameters are presented in Table \ref{tab:DDPGparameters}.

\begin{table}
	\caption{DDPG parameter values} 
	\label{tab:DDPGparameters} 
	\begin{center}
		\begin{tabular}{ p{0.4\textwidth} p{0.2\textwidth} }
			Parameter & Value \\ \hline
			Learning rate & 0.001 \\ 
			Reward discount factor & 0.95 \\ 
			Experience buffer length & 100000 \\ 
			Mini batch size & 32 \\ 			
			Gaussian noise mean & 0.15 \\ 
			Gaussian noise variance & 0.2 \\ 
			Gaussian noise variance decay  & 1e-5 \\ 
			Number of hidden layers & 2\\
			Neurons per hidden layer & 32\\
			

		\end{tabular}
	\end{center}
\end{table}


\subsection{Reward Function}

learning input (leader speed time series)

[also relate parameters to driving style attributes such as desired
speed, accelerations, decelerations, desired time gap, minimum gap]


\section{Model training}
The training of the model comprises two important components, which have to be defined in advance. There is the generation of leading trajectories and the general definition of an training episode, that will be discussed in the following.

\subsection{Generating synthetic leading trajectories}
The leading trajectory is based on an AR(1) process, whose parameters reflect the kinematics of real leaders. The AR(1) process describes the speed of the Leader Vehicle and is defined as 

\begin{equation}
	v_l(t) = c+\phi v_l(t-1)+ \epsilon \text{, with } E(\epsilon) = 0, Var(\epsilon) = \sigma 
\end{equation}

With reaching of stationarity, this process has 
\begin{equation}
\label{eq:E_AR1}
 \text{an expected value of }E(v_l) = \frac{c}{1-\phi}, 
 \end{equation}
 
 \begin{equation}
 \label{eq:V_AR1}
 \text{the variance  }Var(v_l) = \frac{\sigma^2}{1-\phi^2}, 
 \end{equation}

 \begin{equation}
 \label{eq:ACF_AR1}
\text{the autocorrelation  }ACF(dt) = \phi^{dt}, 
\end{equation}

 \begin{equation}
 \label{eq:tau_AR1}
\text{and the correlation time  }\tau = -\frac{1}{ln(\phi)}, 
\end{equation}

with $d$ corresponding to the simulation step size, which is globally set to $100ms$. 

To adjust the parameters of the AR(1) process, typical values for real leader trajectories has to be defined: With $v_{l,des}$ as the desired velocity for the leader, the mean of the AR(1) process is set to be $v_{l,des}/2$ and the standard deviation is set to be $v_{l,des}$. The acceleration $a_{phys}$ corresponds to typical physical leader accelerations. With these values and by using Equation \ref{eq:E_AR1} - \ref{eq:tau_AR1}, the parameters of the AR(1) process can be calculated as:

 \begin{equation}
\phi = e^{(\frac{-2da_{phys}}{v_{l,des}})}
\end{equation}

 \begin{equation}
c=(1-\phi)\frac{v_{l,des}}{2}
\end{equation}

 \begin{equation}
 \sigma^2=(1-\phi^2)\frac{v_{l,des}^2}{4}
\end{equation}

The assumed typical values for $v_{l,des}$ and  $a_{phys}$ as well as the resulting values of the AR(1) process parameters can be found in Table \ref{tab:AR1Parameters}.

\begin{table}
	\caption{Assumed typical values for leading trajectories and the resulting values of the AR(1) process parameters} 
	\label{tab:AR1Parameters} 
	\begin{center}
		\begin{tabular}{ p{0.1\textwidth} p{0.1\textwidth} |p{0.1\textwidth} p{0.1\textwidth}  }
			 \multicolumn{2}{c|}{Real trajectory} & \multicolumn{2}{c}{AR(1) process}   \\ \hline
			$v_{l,des}$ &$15m/s$ &$\phi$ & $0.9933$\\
			$a_{phys}$ &$1m/s^2$ &$c$ & $0.05m/s$\\
			& & $\sigma^2$ & $0.75m^2/s^2$
			
		\end{tabular}
	\end{center}
\end{table}

Figure \ref{fig:AR1process} shows an example trajectory of the leading vehicle based on the AR(1) process using the parameters of Table \ref{tab:AR1Parameters}. If the velocity exceeds the defined range of $[0, v_{l,des}]$, it is manually set to the range limits.
\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{images/AR1process}
	\caption{Example of a leading trajectory based on the parametrized AR1 process used to train the RL agent}
	\label{fig:AR1process}
\end{figure}


\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{images/Training}
	\caption{RL training process, one episode containing 500 steps}
	\label{fig:training}
\end{figure}

\subsection{Training episode definition}

To train the RL agent, a training episode has to be defined. One episode has a simulation time of $50s$ with a step size of $d=100ms$, resulting in a episode length of $500$ steps. The initial velocities of Follower and Leader Vehicle is a randomly set in the range $[0,v_{\rm des}]$ respectively  $[0,v_{l,des}]$. The initial gap between both is set to $120m$. 

\subsection{Results of the RL training process}

Figure \ref{fig:training} shows an example of the training process. The red line shows the moving average reward of the last 30 episodes. After approximately 570 episodes the maximum average reward has been reached. Reaching saturation the learning process has been stopped after 850 episodes. 

\section{Validation}

The goal is not to minimize some error measure as in usual
calibration/validation but to check if the driving style is safe,
effective, and comfortable. The RL strategy is evaluated with respect to these metrics in different driving scenarios, described in the following.

\subsection{Response to an external leading vehicle speed profile}
The first scenario is designed in order to evaluate the transition between free driving and car-following as well as the Follower behavior in safety-critical situations. 
Figure \ref{fig:manipulatedLeader} shows a driving scenario with an external Leading Vehicle speed profile. The initial gap between Follower and Leader is 200 meters, which refers to a free driving scenario. The Follower accelerates with $a_{\rm max} = 2m/s^2$ until the desired speed $v_{\rm des} = 16.6m/s$ is reached and approaches the standing Leader Vehicle. When the gap between both drops below 90 meters, the Follower starts to decelerate with a approximately $b_{\rm comf} = -2m/s^2$ (transition between free driving and car-following) and comes to a standstill with a final gap of approximately $s_{\rm min} = 2m$. In the following the Leader Vehicle makes some random acceleration and deceleration. At the time $t = 40s$ the Leading Vehicle makes a full brake, resulting in a comfortable and safe braking of the following vehicle. The transition between different accelerations happens in a comfortable way, reducing the resulting jerk. 



\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{images/manipulatedLeader.eps}
	\caption{Response to an external leading vehicle speed profile}
	\label{fig:manipulatedLeader}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{images/VarAccComp}
	\caption{Comparison of the acceleration variance between Leader and Follower for AR(1) and Napoli experiment}
	\label{fig:VarAccComp}
\end{figure}






\subsection{String stability}
\label{sec:stringStability}
The second scenario, shown in Figure \ref{fig:AR1Kolonne}, consists of a Leader based on the AR(1) process, followed by five vehicles, each controlled by the trained RL agent. The results show that traffic oscillations can effectively be dampened with a sequence of trained Followers, even if the Leader shows large outliers in acceleration. Figure \ref{fig:VarAccComp} illustrates the difference of accelerations between Leader and the Followers (blue bars). The last Follower shows the lowest variance of acceleration, which demonstrates the ability of the RL agent to flatten the speed profile, to dampen oscillations and thus to increase comfort.  


\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{images/AR1Kolonne}
	\caption{Response to a leader trajectory based on a AR(1) process}
	\label{fig:AR1Kolonne}
\end{figure}


\subsection{Response to a real leader trajectory}

In a further scenario, the abilities of the RL strategy are evaluated with a real leader trajectory. This trajectory comes from experiments in Napoli, where exact data from Leader and Follower were obtained (reference to Punzo et al.). Figure \ref{fig:PunzoKolonne} shows the result of a sequence of five vehicles following a real leader trajectory. Similar to the experiment from Section \ref{sec:stringStability} string stability and the reduction of acceleration variance, shown by the red bars in Figure \ref{fig:VarAccComp}, is demonstrated. At time $t = 140s$ the Leader stands still, and it can be observed, that all following vehicles are keeping the minimum distance $s_{\rm min}$ to the Leader. 


\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{images/PunzoKolonne}
	\caption{Response to a real leader trajectory}
	\label{fig:PunzoKolonne}
\end{figure}


\subsection{Response of different driver characteristics}
\label{sec:differentT}

As mentioned in Section \ref{rewardFunction}, different driving styles can be achieved by adjusting the parameters of the reward function. Three RL agents has been trained on a reward function, that differs in the desired time gap $T_{\rm gap}$ between Follower and Leader Vehicle ($T_{gap,1} = 1.0s$, $T_{gap,2} = 1.5s$, $T_{gap,3} = 2.0s$). Figure \ref{fig:differentT} shows the result of these agents, following the real leader trajectory from Napoli. It can be observed, that a lower value for $T_{\rm gap}$ results in closer driving to the Leader, higher accelerations and decelerations and thus in a more aggressive driving behavior. 

\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{images/differentT}
	\caption{Impact of different parametrized RL agents on driving behavior}
	\label{fig:differentT}
\end{figure}


\subsection{Cross validation with the Intelligent Driver Model}
To compare the performance of the trained RL agent with a classical car-following model, the RL agent and an IDM are calibrated on the same set of parameters. These parameters are obtained from one and the same follower from the Napoli data set (reference) and can be found in Table \ref{tab:IDMparameters}. The IDM is formulated as 

\begin{equation}
a=a_{\rm max}\left(1-\left(\frac{v}{v_{\rm des}}\right)^{4}-\left(\frac{s^{*}\left(v, \Delta v\right)}{s}\right)^{2}\right),
\end{equation}
with
\begin{equation}
s^{*}\left(v, \Delta v\right)=s_{\rm min}+vT_{\rm gap}+\frac{v \Delta v}{2 \sqrt{a_{\rm max} b_{\rm comf}}},
\end{equation}
\begin{equation}
\Delta v = v-v_l
\end{equation}

\begin{table}
	\caption{Parameters obtained from experimental data and used to calibrate the RL agent and the IDM} 
	\label{tab:IDMparameters} 
	\begin{center}
		\begin{tabular}{ p{0.14\textwidth} |p{0.1\textwidth}  } 
		Parameter & Value   \\ \hline
			$T_{\rm gap}$ & $0.83s$\\
			$s_{\rm min}$ & $4.90m$\\
			$a_{\rm max}$ & $4.32m/s^2$\\
			$b_{\rm comf}$ & $2.34 m/s^2$\\
			$v_{\rm des}$ & $33.73m/s$
			
		\end{tabular}
	\end{center}
\end{table}

Figure \ref{fig:IDMvsRL} shows the results for: First, the RL agent, calibrated on the real follower data. Second, the IDM, calibrated on the follower data. And third, the real follower of the Napoli experiment. To evaluate the performance, for both approaches the respective objective function has been computed. The objective function of the RL agent correspondences to the reward function, while the Goodness-of-Fit Function $SSE(s)$ defines the objective function of the IDM. A comparison between RL agent and IDM for both, the reward function and the Goodness-of-Fit Function, is shown in Table \ref{tab:objectiveFunc}. For both objectives the RL agent shows a better performance.

\begin{figure}
	
	\centering
	\includegraphics[width=0.95\textwidth]{images/IDMvsRL_dist}
	\caption{Comparison between IDM and RL agent, calibrated on the same set of parameters}
	\label{fig:IDMvsRL}
\end{figure}

\begin{table}
	\caption{Comparison between calibrated RL agent and IDM for accumulated Reward and Goodness-of-Fit Function $SSE(log(s))$} 
	\label{tab:objectiveFunc} 
	\begin{center}
		\begin{tabular}{p{0.3\textwidth} | p{0.2\textwidth} p{0.2\textwidth}  } 
			& RL agent & IDM   \\ \hline
			$SSE(log(s))$ & $389.10$ &  $418.05$	\\
			Accumulated Reward &  $8.23 \times 10^3$   & $8.08\times 10^3$
			
		\end{tabular}
	\end{center}
\end{table}


\section{Conclusion/Discussion}
evaluation of safety and comfort, comparison to IDM



\bibliography{RL_vehicles_references}

\end{document}
